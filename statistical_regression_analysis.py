# -*- coding: utf-8 -*-
"""Statistical-Regression-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aiobvsLFzqnrvPCzQobJgNhjVawsAppV

## Import Packages and Load Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# For data manipulation
import pandas as pd 
import numpy as np

# For data visualization
import matplotlib.pyplot as plt 
import seaborn as sns

# Relevant models and tools
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error # Calculate MSE (cost function)
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
# %matplotlib inline

# Load data
df = pd.read_csv('winequality-red.csv')

# Show shape of data
print(f'Shape of dataset (rows, columns): {df.shape}')

df.head()

"""## Data Visualization and Preprocessing"""

# Check for missing values
print(f'\nNumber of null values in each column: \n{df.isnull().sum()}\n') # apparently, there are no null values in the dataset.

plt.figure(figsize=(8,7))
sns.countplot(x="quality", data=df)  
plt.title("WINE QUALITY",fontsize=14)
plt.show()

# Split data into features and output variable
data_X = df.drop(['quality'], axis=1).values
data_y = df['quality'].values

"""## Split Dataset and Feature Scaling"""

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=.20, random_state=42)

# Perform feature scaling using standardization
scaler = StandardScaler().fit(X_train)
X_train_normalized = scaler.transform(X_train)
X_test_normalized = scaler.transform(X_test)

# Plot coefficients
def plot_errors(lambdas, train_errors, test_errors, title):
  plt.figure(figsize=(14, 8))
  plt.plot(lambdas, train_errors, label="train")
  plt.plot(lambdas, test_errors, label="test")
  plt.xlabel("$\\lambda$", fontsize=14)
  plt.ylabel("MSE", fontsize=14)
  plt.title(title, fontsize=20)
  plt.legend(fontsize=14)
  plt.show()

"""## Linear Regression with Lasso (L1) Regularization"""

def lasso_model(lambdas):
  training_errors = [] # we will store the error on the training set, for using each different lambda
  testing_errors = [] # and the error on the testing set

  for l in lambdas:
    # in sklearn, they refer to lambda as alpha, the name is different in different literature
    # Model is Lasso
    model = Lasso(alpha=l, max_iter=1000) # we allow max number of iterations until the model converges
    model.fit(X_train_normalized, y_train)

    training_predictions = model.predict(X_train_normalized)
    training_mse = mean_squared_error(y_train, training_predictions)
    training_errors.append(training_mse)

    testing_predictions = model.predict(X_test_normalized)
    testing_mse = mean_squared_error(y_test, testing_predictions)
    testing_errors.append(testing_mse)

    print(f'Coefficient when alpha is {l}: {model.coef_}\n')

  return training_errors, testing_errors

# let's generate different values for lambda/alpha
lambdas = np.arange(0.01, 1.0, step=0.1)

lasso_train, lasso_test = lasso_model(lambdas)
plot_errors(lambdas, lasso_train, lasso_test, "Lasso (L1)")

"""## Linear Regression with Ridge (L2) Regularization"""

def ridge_model(lambdas):
  training_errors = [] # we will store the error on the training set, for using each different lambda
  testing_errors = [] # and the error on the testing set

  for l in lambdas:
    # in sklearn, they refer to lambda as alpha, the name is different in different literature
    # Model is Ridge or ElasticNet
    model = Ridge(alpha=l, max_iter=1000) # we allow max number of iterations until the model converges
    model.fit(X_train_normalized, y_train)

    training_predictions = model.predict(X_train_normalized)
    training_mse = mean_squared_error(y_train, training_predictions)
    training_errors.append(training_mse)

    testing_predictions = model.predict(X_test_normalized)
    testing_mse = mean_squared_error(y_test, testing_predictions)
    testing_errors.append(testing_mse)

    print(f'Coefficient when alpha is {l}: {model.coef_}\n')

  return training_errors, testing_errors

# let's generate different values for lambda/alpha
lambdas = np.arange(0, 3, step=0.1)

ridge_train, ridge_test = ridge_model(lambdas)
plot_errors(lambdas, ridge_train, ridge_test, "Ridge (L2)")

"""## Linear Regression with Elastic Regularization"""

def elastic_net_model(lambdas):
  training_errors = [] # we will store the error on the training set, for using each different lambda
  testing_errors = [] # and the error on the testing set

  for l in lambdas:
    # in sklearn, they refer to lambda as alpha, the name is different in different literature
    # Model is Elastic Net
    model = ElasticNet(alpha=l, max_iter=1000) # we allow max number of iterations until the model converges
    model.fit(X_train_normalized, y_train)

    training_predictions = model.predict(X_train_normalized)
    training_mse = mean_squared_error(y_train, training_predictions)
    training_errors.append(training_mse)

    testing_predictions = model.predict(X_test_normalized)
    testing_mse = mean_squared_error(y_test, testing_predictions)
    testing_errors.append(testing_mse)

    print(f'Coefficient when alpha is {l}: {model.coef_}\n')

  return training_errors, testing_errors

# let's generate different values for lambda/alpha
lambdas = np.arange(0.01, 1.0, step=0.1)

elastic_train, elastic_test = elastic_net_model(lambdas)
plot_errors(lambdas, elastic_train, elastic_test, "Elastic Net")

"""## Support Vector Regression"""

# Support Vector Regression (SVR) model
regressor = SVR(kernel='rbf', C=10)
regressor.fit(X_train_normalized, y_train)

y_pred = regressor.predict(X_test_normalized)

# RMSE (Root Mean Squared Error) and Mean Squared Error
rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))
mse = float(mean_squared_error(y_test, y_pred))
print(f'\nRMSE: {rmse:.4f}\nMSE: {mse:.4f}')